#================================åŒ¯å…¥å‡½æ•¸===============================
import pandas as pd
import numpy as np
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV
from sklearn.metrics import accuracy_score, confusion_matrix, classification_report,roc_curve, roc_auc_score
from xgboost import XGBClassifier
import matplotlib.pyplot as plt
import seaborn as sns
import shap
import os
import joblib
from sklearn.model_selection import StratifiedKFold
saving_file = "regular_season/XGBoost_image"#æª”æ¡ˆå„²å­˜åç¨±
#======================================================================
df = pd.read_csv(r"D:\AI_prediction\python_program\program1\NBA_2021_to_2024_regular_season.csv", encoding="utf-8-sig")
df = np.round(df, 3) # æ”¹è®Šè³‡æ–™åªåˆ°å°æ•¸ç¬¬ä¸‰ä½

X = df.drop(columns=["result"], axis=1)  # ç‰¹å¾µæ¬„ä½ï¼Œaxis=1(æ¬„ä½)åˆªæ‰resultæ¬„ä½ã€‚
y = df["result"]                 # æ¨™ç±¤æ¬„ä½

#================================åˆ†å‰²è³‡æ–™ç‚ºè¨“ç·´é›†å’Œæ¸¬è©¦é›†===============================
# stratify=yæŒ‰ç…§yçš„åˆ†å¸ƒä¾†åˆ‡åˆ†è³‡æ–™ï¼Œä¿æŒè¨“ç·´å’Œæ¸¬è©¦è³‡æ–™çš„å‹è² æ¯”ä¾‹ç›¸åŒã€‚
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42, stratify=y
) 
#===================================================================================

#================================å»ºç«‹æ¨¡å‹===============================
# å»ºç«‹åŸºæœ¬æ¨¡å‹
xgb_model = XGBClassifier(objective='binary:logistic', eval_metric='logloss', random_state=42)

# å®šç¾©è¦å˜—è©¦çš„åƒæ•¸çµ„åˆ
param_grid = {
    'n_estimators': [100, 200],    # æ¨¹çš„æ•¸é‡
    'max_depth': [3, 5, 7],        # æ¨¹æ·±åº¦(å±¤æ•¸)
    'learning_rate': [0.01, 0.1],  # å­¸ç¿’ç‡
    'subsample': [0.8, 1.0],       #æ±ºå®šæ¯æ£µæ¨¹è¨“ç·´æ™‚ä½¿ç”¨çš„æ¨£æœ¬æ¯”ä¾‹(æ¨£æœ¬æŠ½æ¨£ã€‚ä¾‹:å‹‡å£«éšŠ...)
    'colsample_bytree': [0.8, 1.0] #æ§åˆ¶æ¯æ£µæ¨¹è¨“ç·´æ™‚éš¨æ©Ÿé¸å–ç‰¹å¾µçš„æ¯”ä¾‹ã€‚(ç‰¹å¾µæŠ½æ¨£ã€‚ä¾‹:æŠ•çƒæ•¸ã€å¤±èª¤...)
}

# å»ºç«‹ GridSearchCV ç‰©ä»¶
grid_search = GridSearchCV(
    estimator=xgb_model,      # è¦èª¿çš„æ¨¡å‹
    param_grid=param_grid,    # è¦å˜—è©¦çš„åƒæ•¸çµ„åˆ
    cv=5,                     # ä½¿ç”¨ 5 æŠ˜äº¤å‰é©—è­‰
    scoring='accuracy',       # ç”¨æº–ç¢ºç‡è©•ä¼°
    n_jobs=-1,                # ç”¨æ‰€æœ‰ CPU æ ¸å¿ƒåŠ é€Ÿ(å› ç‚ºå¤šæ ¸å¿ƒéœ€è·‘å¤šå€‹è³‡æ–™æ‰€ä»¥éœ€è¦tempæš«å­˜è³‡æ–™å¤¾ä¾†å­˜æ±è¥¿ï¼Œä½†åŸæœ¬çš„tempå› ç‚ºåœ¨ä¸­æ–‡è·¯å¾‘ä¸‹æœƒå‡ºéŒ¯ï¼Œæ‰€ä»¥æŒ‡å®šæ–°çš„æš«å­˜è³‡æ–™å¤¾è·¯å¾‘çµ¦temp_folder)
    verbose=2                 # é¡¯ç¤ºé€²åº¦
)

# é–‹å§‹æœå°‹æœ€ä½³åƒæ•¸
temp_folder = r"D:\temp_joblib"  # å…¨è‹±æ–‡è·¯å¾‘
with joblib.parallel_backend('loky', temp_folder=temp_folder):
    grid_search.fit(X_train, y_train)

# é¡¯ç¤ºæœ€ä½³çµæœ
print("æœ€ä½³åƒæ•¸çµ„åˆï¼š", grid_search.best_params_)

# å–å‡ºæœ€ä½³åƒæ•¸
best_params = grid_search.best_params_

xgb_best_model = XGBClassifier(
    objective='binary:logistic',  # äºŒå…ƒåˆ†é¡
    eval_metric='logloss',        # æå¤±å‡½æ•¸           
    random_state=42,              # éš¨æ©Ÿæ€§ç¨®å­
    **best_params                 # æŠŠæœ€ä½³åƒæ•¸å‚³å…¥
)
#======================================================================

#================================è¨“ç·´æ¨¡å‹===============================
xgb_best_model.fit(X_train, y_train)
#======================================================================

#================================å°å‡ºè³‡è¨Š======================================
# é æ¸¬
y_pred = xgb_best_model.predict(X_test)

# æº–ç¢ºç‡
acc = accuracy_score(y_test, y_pred)
print("Accuracy:", acc)

# æ··æ·†çŸ©é™£
cm = confusion_matrix(y_test, y_pred) # è¨ˆç®—æ··æ·†çŸ©é™£
print("Confusion Matrix:\n", cm)

# è©³ç´°åˆ†é¡å ±å‘Š
print(classification_report(y_test, y_pred, digits=4))
#=============================================================================

#================================5-fold cross-validation======================
kf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)

recall_1_list = []
precision_1_list = []
f1_1_list = []
accuracy_list = []
X_np = X.to_numpy()
y_np = y.to_numpy()

for train_idx, test_idx in kf.split(X_np, y_np):
    X_train_fold, X_test_fold = X_np[train_idx], X_np[test_idx]
    y_train_fold, y_test_fold = y_np[train_idx], y_np[test_idx]

    # è¨“ç·´æ¨¡å‹
    xgb_best_model.fit(X_train_fold, y_train_fold)

    # é æ¸¬
    y_pred_fold = xgb_best_model.predict(X_test_fold)

    # æ··æ·†çŸ©é™£
    cm = confusion_matrix(y_test_fold, y_pred_fold)
    TN, FP, FN, TP = cm.ravel()

    # Precisionã€Recall for class 1
    recall_1 = TP / (TP + FN) if (TP + FN) > 0 else 0
    precision_1 = TP / (TP + FP) if (TP + FP) > 0 else 0

    recall_1_list.append(recall_1)
    precision_1_list.append(precision_1)

    # F1-score
    if (precision_1 + recall_1) > 0:
        f1_1 = 2 * precision_1 * recall_1 / (precision_1 + recall_1)
    else:
        f1_1 = 0
    f1_1_list.append(f1_1)

    # Accuracyï¼ˆæ•´é«”æº–ç¢ºç‡ï¼‰
    accuracy = (TP + TN) / (TP + TN + FP + FN)
    accuracy_list.append(accuracy)

print("æ¯ä¸€ fold çš„ Accuracy:", accuracy_list)
print("å¹³å‡ Accuracy:", np.mean(accuracy_list))

print("\næ¯ä¸€ fold çš„ Recall (class 1):", recall_1_list)
print("å¹³å‡ Recall_1:", np.mean(recall_1_list))

print("\næ¯ä¸€ fold çš„ Precision (class 1):", precision_1_list)
print("å¹³å‡ Precision_1:", np.mean(precision_1_list))

print("\næ¯ä¸€ fold çš„ F1-score (class 1):", f1_1_list)
print("å¹³å‡ F1-score_1:", np.mean(f1_1_list))
#=============================================================================



#================================æ··æ·†çŸ©é™£åœ–======================
plt.figure(figsize=(6,4)) # å¯¬6å‹ ã€ é«˜4å‹
sns.heatmap(cm, annot=True, fmt="d", cmap="Blues") # sns.heatmap()ç•«ç†±åŠ›åœ–ç”¨ ã€ annot=True(æ¯å€‹æ ¼å­ä¸­æ˜¯å¦é¡¯ç¤ºæ•¸æ“š) ã€ fmt="d"(æ ¼å­ä¸­çš„æ•¸å­—é¡¯ç¤ºæ•´æ•¸) ã€ cmap="Blues"(é¡¯ç¤ºåœ–ç‚ºè—è‰²ä¸»é¡Œ)
plt.title("Confusion Matrix")
plt.xlabel("Predicted Label")
plt.ylabel("True Label")
plt.tight_layout()
os.makedirs(saving_file, exist_ok=True)  # è‹¥è³‡æ–™å¤¾ä¸å­˜åœ¨æœƒè‡ªå‹•å»ºç«‹
plt.savefig(os.path.join(saving_file, "confusion_matrix.png"), dpi=150)
plt.close()
print("Confusion matrix saved as confusion_matrix.png\n")
#==============================================================

#===========================5-fold æº–ç¢ºç‡åœ–==================
plt.figure(figsize=(6,4))
plt.bar(range(1,6), recall_1_list) #plt.bar()ç•«é•·æ¢åœ–ç”¨ ã€ range(1,6)é€™å€‹æ˜¯Xè»¸åº§æ¨™ ã€ scoresé€™å€‹æ˜¯yè»¸åº§æ¨™(5æ¬¡çš„æº–ç¢ºç‡)
plt.title("5-Fold Cross-Validation Accuracy")
plt.xlabel("Fold")
plt.ylabel("Accuracy")
plt.ylim(0,1)      # è¨­å®š y è»¸çš„ç¯„åœï¼ˆä¸Šä¸‹é™ï¼‰
plt.tight_layout() # è‡ªå‹•èª¿æ•´åœ–è¡¨çš„ç©ºé–“é…ç½®
os.makedirs(saving_file, exist_ok=True)  # è‹¥è³‡æ–™å¤¾ä¸å­˜åœ¨æœƒè‡ªå‹•å»ºç«‹
plt.savefig(os.path.join(saving_file, "cross_val_accuracy.png"), dpi=150)
plt.close()
print("Cross-validation plot saved as cross_val_accuracy.png\n")
#=========================================================

#==================================ROC Curveåœ–=====================================
y_prob = xgb_best_model.predict_proba(X_test)[:, 1] # y_probæ¨¡å‹é æ¸¬è´çš„æ©Ÿç‡

fpr, tpr, thresholds = roc_curve(y_test, y_prob)
auc = roc_auc_score(y_test, y_prob)

plt.figure(figsize=(6,5))
plt.plot(fpr, tpr, label=f"XGBoost (AUC = {auc:.4f})")
plt.plot([0,1], [0,1], 'k--', label="Random")
plt.xlabel("False Positive Rate")
plt.ylabel("True Positive Rate")
plt.title("ROC Curve")
plt.legend() # é¡¯ç¤ºåœ–ä¾‹
plt.tight_layout() # è‡ªå‹•èª¿æ•´åœ–è¡¨çš„ç©ºé–“é…ç½®
os.makedirs(saving_file, exist_ok=True)  # è‹¥è³‡æ–™å¤¾ä¸å­˜åœ¨æœƒè‡ªå‹•å»ºç«‹
plt.savefig(os.path.join(saving_file, "ROC_curve.png"), dpi=150)
plt.close()
print("ROC_curve plot saved as ROC_curve.png\n")
#==============================================================================

#==================================ç‰¹å¾µé‡è¦åº¦åœ–=====================================
importance = xgb_best_model.feature_importances_ # å–å¾—æ¯å€‹ç‰¹å¾µçš„é‡è¦åº¦åˆ†æ•¸ï¼ˆ0~1ä¹‹é–“ï¼‰
features = X.columns # å–å¾—ç‰¹å¾µåç¨±

# æ’åºå¾Œç•«åœ–
indices = np.argsort(importance)[::-1]

plt.figure(figsize=(10,6))
plt.bar(range(len(features)), importance[indices]) # range(len(features))ç‰¹å¾µæ•¸é‡ ã€ importance[indices]æŒ‰ç…§æ’åºå¾Œçš„é‡è¦åº¦ç´¢å¼•å–å‡ºæ•¸å€¼ç•«å‡ºé•·æ¢åœ–ã€‚
plt.xticks(range(len(features)), features[indices], rotation=90) # range(len(features))ç‰¹å¾µæ•¸é‡ ã€ features[indices]ç”±ç‰¹å¾µæ¬Šé‡å¤§åˆ°å°æ’åºåˆ°Xè»¸(ç‰¹å¾µåç¨±) ã€ rotation=90æŠŠæ–‡å­—æ—‹è½‰ 90 åº¦ï¼ˆç›´ç«‹é¡¯ç¤ºï¼‰
plt.title("XGBoost Feature Importance")
plt.tight_layout()
os.makedirs(saving_file, exist_ok=True)  # è‹¥è³‡æ–™å¤¾ä¸å­˜åœ¨æœƒè‡ªå‹•å»ºç«‹
plt.savefig(os.path.join(saving_file, "Feature_importance.png"), dpi=150)
plt.close()
print("Feature_importance plot saved as Feature_importance.png\n")
#=================================================================================

#==================================SHAPè§£é‡‹=====================================
explainer = shap.TreeExplainer(xgb_best_model)
shap_values = explainer.shap_values(X_test)

# ----------------------------
# 1. å…¨åŸŸç‰¹å¾µé‡è¦åº¦ (bar plot)ã€‚ Bar plot æ˜¯ å–å¹³å‡çµ•å°å€¼ï¼Œæ‰€ä»¥åªé¡¯ç¤ºã€Œå½±éŸ¿åŠ›å¤§å°ã€ï¼Œä¸æœƒé¡¯ç¤ºå¢åŠ æˆ–é™ä½å‹ç‡ã€‚
#å‡è¨­ç‰¹å¾µ ğ‘— çš„ 5 å€‹æ¨£æœ¬ SHAP å€¼å¦‚ä¸‹ï¼š
#Ï•jâ€‹=[0.2,âˆ’0.3,0.1,âˆ’0.1,0.5] ç›¸åŒç‰¹å¾µçš„SHAPå€¼ï¼Œâˆ£0.2âˆ£ + âˆ£-0.3âˆ£ + âˆ£0.1âˆ£ + âˆ£-0.1âˆ£ + âˆ£0.5âˆ£â€‹ / 5 = 0.24
# ----------------------------
plt.figure()
shap.summary_plot(shap_values, X_test, plot_type="bar", show=False)  # show=False ä¸ç›´æ¥é¡¯ç¤º
os.makedirs(saving_file, exist_ok=True)  # è‹¥è³‡æ–™å¤¾ä¸å­˜åœ¨æœƒè‡ªå‹•å»ºç«‹
plt.savefig(os.path.join(saving_file, "shap_summary_bar.png"), dpi=150, bbox_inches='tight')
plt.close()  # é‡‹æ”¾åœ–å½¢è³‡æº
print("shap_summary_bar plot saved as shap_summary_bar.png\n")

# ----------------------------
# 2. è©³ç´° SHAP åˆ†ä½ˆ (dot plot)
# ----------------------------
plt.figure()
shap.summary_plot(shap_values, X_test, show=False)
os.makedirs(saving_file, exist_ok=True)  # è‹¥è³‡æ–™å¤¾ä¸å­˜åœ¨æœƒè‡ªå‹•å»ºç«‹
plt.savefig(os.path.join(saving_file, "shap_summary_dot.png"), dpi=150, bbox_inches='tight')
plt.close()  # é‡‹æ”¾åœ–å½¢è³‡æº
print("shap_summary_dot plot saved as shap_summary_dot.png\n")
#===============================================================================